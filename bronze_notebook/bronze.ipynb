{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e44492be-19a4-4bdc-9126-e99269a46ca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Batch Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "047bae6c-b594-4a2c-8fe3-e7c6605b43db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## import and fedine schema \n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "airlines_schema = StructType([\n",
    "    StructField(\"IATA_CODE\", StringType(), True),\n",
    "    StructField(\"AIRLINE\", StringType(), True),\n",
    "    StructField(\"LOAD_TIMESTAMP\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "airports_schema = StructType([\n",
    "    StructField(\"IATA_CODE\", StringType(), True),\n",
    "    StructField(\"AIRPORT\", StringType(), True),\n",
    "    StructField(\"CITY\", StringType(), True),\n",
    "    StructField(\"STATE\", StringType(), True),\n",
    "    StructField(\"COUNTRY\", StringType(), True),\n",
    "    StructField(\"LATITUDE\", StringType(), True),\n",
    "    StructField(\"LONGITUDE\", StringType(), True),\n",
    "    StructField(\"LOAD_TIMESTAMP\", TimestampType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c5677a-ca68-44da-a5c6-829cf681c35e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema_map = {\n",
    "    \"airlines.csv\": airlines_schema,\n",
    "    \"airports.csv\": airports_schema\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d595bdc4-4c52-4044-a382-41d29a1bbbd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch file: airlines.csv\nSaved Bronze table: airlines\nProcessing batch file: airports.csv\nSaved Bronze table: airports\nSkipping non-batch file: flights/\n"
     ]
    }
   ],
   "source": [
    "## Define paths and process batch data\n",
    "\n",
    "raw_path = \"dbfs:/FileStore/vijay_project/raw\"\n",
    "bronze_path = \"dbfs:/FileStore/vijay_project/bronze/batch_data\"\n",
    "\n",
    "# List all files in RAW folder\n",
    "raw_files = dbutils.fs.ls(raw_path)\n",
    "\n",
    "for file in raw_files:\n",
    "    file_name = file.name\n",
    "    \n",
    "    # process only if schema exists for this file\n",
    "    if file_name in schema_map:\n",
    "        print(f\"Processing batch file: {file_name}\")\n",
    "        \n",
    "        # Read with schema\n",
    "        df = (\n",
    "            spark.read\n",
    "            .option(\"header\", \"true\")\n",
    "            .schema(schema_map[file_name])\n",
    "            .csv(f\"{raw_path}/{file_name}\")\n",
    "        )\n",
    "\n",
    "        # Add metadata\n",
    "        from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "        df_bronze = df.withColumn(\"INGESTED_AT\", current_timestamp()) \\\n",
    "                      .withColumn(\"SOURCE_FILE\", input_file_name())\n",
    "\n",
    "        # Save to Bronze\n",
    "        table_name = file_name.replace(\".csv\", \"\")\n",
    "\n",
    "        df_bronze.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "            .save(f\"{bronze_path}/{table_name}\")\n",
    "\n",
    "        print(f\"Saved Bronze table: {table_name}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping non-batch file: {file_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49b49a86-a40a-4a7f-bfe0-c1733c18d9b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf706b72-6a0c-48a3-bf01-70d58b368e13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## imports\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "from pyspark.sql.types import (\n",
    "\n",
    "    StructType, StructField,\n",
    "\n",
    "    IntegerType, StringType, TimestampType\n",
    "\n",
    ")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bb38cc2-be8d-4af9-92a4-920fb7c3dbf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## define schema \n",
    "\n",
    "flights_schema = StructType([\n",
    "    StructField(\"YEAR\", IntegerType(), True),\n",
    "    StructField(\"MONTH\", IntegerType(), True),\n",
    "    StructField(\"DAY\", IntegerType(), True),\n",
    "    StructField(\"DAY_OF_WEEK\", IntegerType(), True),\n",
    "    StructField(\"AIRLINE\", StringType(), True),\n",
    "    StructField(\"FLIGHT_NUMBER\", StringType(), True),\n",
    "    StructField(\"TAIL_NUMBER\", StringType(), True),\n",
    "    StructField(\"ORIGIN_AIRPORT\", StringType(), True),\n",
    "    StructField(\"DESTINATION_AIRPORT\", StringType(), True),\n",
    "    StructField(\"SCHEDULED_DEPARTURE\", StringType(), True),\n",
    "    StructField(\"DEPARTURE_TIME\", StringType(), True),\n",
    "    StructField(\"DEPARTURE_DELAY\", StringType(), True),\n",
    "    StructField(\"TAXI_OUT\", StringType(), True),\n",
    "    StructField(\"WHEELS_OFF\", StringType(), True),\n",
    "    StructField(\"SCHEDULED_TIME\", StringType(), True),\n",
    "    StructField(\"ELAPSED_TIME\", StringType(), True),\n",
    "    StructField(\"AIR_TIME\", StringType(), True),\n",
    "    StructField(\"DISTANCE\", IntegerType(), True),\n",
    "    StructField(\"WHEELS_ON\", StringType(), True),\n",
    "    StructField(\"TAXI_IN\", StringType(), True),\n",
    "    StructField(\"SCHEDULED_ARRIVAL\", StringType(), True),\n",
    "    StructField(\"ARRIVAL_TIME\", StringType(), True),\n",
    "    StructField(\"ARRIVAL_DELAY\", StringType(), True),\n",
    "    StructField(\"DIVERTED\", StringType(), True),\n",
    "    StructField(\"CANCELLED\", StringType(), True),\n",
    "    StructField(\"CANCELLATION_REASON\", StringType(), True),\n",
    "    StructField(\"AIR_SYSTEM_DELAY\", StringType(), True),\n",
    "    StructField(\"SECURITY_DELAY\", StringType(), True),\n",
    "    StructField(\"AIRLINE_DELAY\", StringType(), True),\n",
    "    StructField(\"LATE_AIRCRAFT_DELAY\", StringType(), True),\n",
    "    StructField(\"WEATHER_DELAY\", StringType(), True),\n",
    "    StructField(\"LOAD_TIMESTAMP\", TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c489739-42d2-43bf-b9e6-5c609a5660a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## define paths\n",
    "\n",
    "raw_path = \"dbfs:/FileStore/vijay_project/raw/flights\"\n",
    "bronze_path = \"dbfs:/FileStore/vijay_project/bronze/streaming_data/flights\"\n",
    "checkpoint_path = \"dbfs:/FileStore/vijay_project/check_point/flights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bae9ecf6-6c60-4b0d-86cf-059d33340755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze Exists: True\n"
     ]
    }
   ],
   "source": [
    "## Function to check if a Delta table exists at the given path, and then validate if the bronze layer is created\n",
    "\n",
    "def delta_exists(path):\n",
    "    try:\n",
    "        dbutils.fs.ls(path + \"/_delta_log\")\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "bronze_exists = delta_exists(bronze_path)\n",
    "print(\"Bronze Exists:\", bronze_exists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "840be118-4612-4130-8fa9-3b861fbf4670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Validating the schema \n",
    "\n",
    "def validate_schema(incoming_schema, existing_schema):\n",
    "    incoming_cols = {f.name: f.dataType for f in incoming_schema.fields}\n",
    "    existing_cols = {f.name: f.dataType for f in existing_schema.fields}\n",
    "\n",
    "    # Columns added later (should not be validated)\n",
    "    ignore_cols = {\"INGESTED_AT\", \"SOURCE_FILE\"}\n",
    "\n",
    "    for col, dtype in existing_cols.items():\n",
    "        if col in ignore_cols:\n",
    "            continue\n",
    "        if col not in incoming_cols:\n",
    "            raise Exception(f\"Missing required column: {col}\")\n",
    "        if incoming_cols[col] != dtype:\n",
    "            raise Exception(f\"Data type mismatch for column: {col}\")\n",
    "\n",
    "    print(\"Schema validation passed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08126d79-0ece-4dcc-a625-40f38204e10f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema validation passed\n"
     ]
    }
   ],
   "source": [
    "# If the Bronze table already exists, load it and validate its schema\n",
    "\n",
    "if bronze_exists:\n",
    "    existing_df = spark.read.format(\"delta\").load(bronze_path)\n",
    "    validate_schema(flights_schema, existing_df.schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0c1e1f8-60b8-4957-bbdb-a2949ffc10b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Define the streaming DataFrame by reading CSV files from the raw path\n",
    "\n",
    "stream_df = (\n",
    "    spark.readStream\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(flights_schema)   # Mandatory\n",
    "    .load(raw_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8121af1f-76fd-4dd3-bcbc-38fb32000398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Add metadata columns to the streaming DataFrame\n",
    " \n",
    "bronze_stream_df = (\n",
    "    stream_df\n",
    "    .withColumn(\"INGESTED_AT\", current_timestamp())\n",
    "    .withColumn(\"SOURCE_FILE\", input_file_name())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25b88d27-0b60-4254-92cd-e3988051a7c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f6679b7b920>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Write the streaming DataFrame to the Bronze table\n",
    "\n",
    "(\n",
    "    bronze_stream_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .option(\"mergeSchema\", \"true\")   # handles new columns safely\n",
    "    .trigger(once=True)\n",
    "    .start(bronze_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6133370-f791-4625-8479-08fee22bd7c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ce7329d-9879-4b2b-ad2a-670511143ef6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5819079"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"delta\") \\\n",
    "    .load(bronze_path) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7436619-9067-42da-84f8-c5555a82c5df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}